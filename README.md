# ğŸš€ Neura-IQ Multimodal AI Research Assistant

**Neura-IQ Multimodal AI Research Assistant** is a locally-running multimodal AI system that lets you ingest PDF documents (text, tables, and images), chat with them using RAG, and fine-tune a lightweight LLM on your own data â€” all with zero cloud dependencies.

---

## âœ¨ Features

- **ğŸ“š Multimodal PDF Ingestion** â€” Extracts text, tables, and images from PDFs using `unstructured` (HI_RES strategy)
- **ğŸ” Multimodal RAG** â€” Retrieves text, table, and image chunks from ChromaDB and answers questions using LLaVA 13B
- **ğŸ¤– General LLM Chat** â€” Free-form chat with LLaVA 13B, supports image uploads
- **ğŸ§  Fine-Tuning** â€” Auto-generates Q&A pairs from your indexed documents and fine-tunes **FLAN-T5-small** using **LoRA (PEFT)** â€” fully on CPU
- **ğŸ” Auth System** â€” Login page, role-based access (admin/user), session timeout, brute-force lockout, user management panel
- **ğŸ—ƒï¸ Fully Local** â€” Ollama (LLaVA + Nomic embeddings), ChromaDB, SQLite â€” no API keys, no cloud

---

## ğŸ—ï¸ Architecture

```
User (Browser)
    â”‚
    â–¼
Streamlit App (core.py)
    â”‚
    â”œâ”€â”€â”€ Auth Layer (auth_ui.py + auth_utils.py)
    â”‚         â””â”€â”€ SQLite (users.db) â† bcrypt + session timeout
    â”‚
    â””â”€â”€â”€ EnhancedMultimodalRAG
              â”œâ”€â”€ Ollama (local)
              â”‚     â”œâ”€â”€ llava:13b        â† generation + image analysis
              â”‚     â””â”€â”€ nomic-embed-text â† embeddings
              â”œâ”€â”€ ChromaDB (persistent)
              â”‚     â””â”€â”€ "multimodal_rag" collection
              â”œâ”€â”€ unstructured.io
              â”‚     â””â”€â”€ PDF â†’ text + tables + images
              â””â”€â”€ HuggingFace (CPU)
                    â”œâ”€â”€ FLAN-T5-small (base)
                    â””â”€â”€ LoRA fine-tuned T5
```

---

## ğŸ“‹ Requirements

- Python 3.10+
- [Ollama](https://ollama.com) installed and running
- Required Ollama models:
  ```bash
  ollama pull llava:13b
  ollama pull nomic-embed-text
  ```
- System dependencies for `unstructured` PDF extraction:
  ```bash
  sudo apt install tesseract-ocr poppler-utils
  ```

---

## ğŸš€ Setup & Run

```bash
# 1. Clone the repo
git clone https://github.com/Torque4708/Neura-IQ.git
cd Neura-IQ

# 2. Create and activate a virtual environment
python -m venv env
source env/bin/activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Make sure Ollama is running
ollama serve

# 5. Launch the app
streamlit run core.py --server.port 8501
```

Or use the boot script (update paths first):
```bash
bash neura_iq_boot.sh
```

---

## ğŸ” Default Login

| Username | Password | Role  |
|----------|----------|-------|
| `anand`  | `anand123` | Admin |

> Change the default password in `auth_utils.py` before sharing or deploying.

---

## ğŸ“‚ Data Storage

| Path | Contents |
|---|---|
| `data/pdfs/` | Uploaded PDF files |
| `data/figures/<pdf_name>/` | Extracted images per PDF |
| `data/tables/<pdf_name>/` | Extracted tables as CSVs |
| `data/chromadb/` | ChromaDB vector store |
| `data/metadata.json` | PDF processing metadata |
| `data/finetuned_t5_lora/` | Saved fine-tuned model |
| `data/qa_dataset.json` | Generated Q&A pairs |
| `users.db` | SQLite user database |

---

## ğŸ§  Fine-Tuning Pipeline

1. **Index your PDFs** via the *Create New Index* mode
2. Go to **Fine-Tune Model** â†’ *Generate Q&A* tab â€” LLaVA auto-generates question-answer pairs from your chunks
3. Switch to *Fine-Tune* tab â€” trains FLAN-T5-small with LoRA on CPU
4. In the *Test Model* tab â€” compare **Base T5** vs **Fine-tuned T5** side-by-side

**LoRA Config:** rank=16, alpha=32, targets all attention and FFN projections (`q, k, v, o, wi_0, wi_1, wo`)

---

## ğŸ—‚ï¸ Project Structure

```
Neura-IQ/
â”œâ”€â”€ core.py            # Main app â€” RAG engine, fine-tuning, all UI modes
â”œâ”€â”€ auth_utils.py      # Auth backend â€” SQLite, bcrypt, session management
â”œâ”€â”€ auth_ui.py         # Auth frontend â€” login page, user management UI
â”œâ”€â”€ requirements.txt   # Python dependencies
â”œâ”€â”€ neura_iq_boot.sh   # Shell launcher (update paths before use)
â””â”€â”€ data/              # Auto-created at runtime
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology |
|---|---|
| UI | Streamlit |
| LLM + Vision | LLaVA 13B via Ollama |
| Embeddings | Nomic Embed Text via Ollama |
| Vector Store | ChromaDB |
| PDF Parsing | unstructured (HI_RES) |
| Fine-tuning | HuggingFace Transformers + PEFT/LoRA |
| Base model | google/flan-t5-small |
| Auth DB | SQLite + bcrypt (passlib) |

---

## ğŸ“„ License

MIT License â€” feel free to use, modify, and build on it.
